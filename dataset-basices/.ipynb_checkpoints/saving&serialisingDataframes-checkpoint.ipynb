{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62218e9d",
   "metadata": {},
   "source": [
    "# Saving and Serialising the DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c73ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imorting libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b6e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a random dataset\n",
    "\n",
    "df = pd.DataFrame(data = np.random.random(size=(100000,5)),columns = ['A','B','C','D','F'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33c5b8f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/compat/_optional.py:138\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# df.to_pickle('Save.pkl')# pkl file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_hdf(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSave.hdf\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m# dhf file\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave,fth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:2681\u001b[0m, in \u001b[0;36mDataFrame.to_feather\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2657\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary Feather format.\u001b[39;00m\n\u001b[1;32m   2658\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[38;5;124;03msupports custom indices e.g. `to_parquet`.\u001b[39;00m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeather_format\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_feather\n\u001b[0;32m-> 2681\u001b[0m \u001b[43mto_feather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/feather_format.py:51\u001b[0m, in \u001b[0;36mto_feather\u001b[0;34m(df, path, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_feather\u001b[39m(\n\u001b[1;32m     30\u001b[0m     df: DataFrame,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Write a DataFrame to the binary Feather format.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.1.0\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feather\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, DataFrame):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/compat/_optional.py:141\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'pyarrow'.  Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "# Saving dataset into csv file\n",
    "\n",
    "df.to_csv('Save.csv',index=False, float_format='%.4f')# csv file\n",
    "\n",
    "# df.to_pickle('Save.pkl')# pkl file\n",
    "\n",
    "df.to_hdf('Save.hdf', key= 'data' , format='table')# dhf file\n",
    "\n",
    "df.to_feather('save,fth') # feather file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e71df",
   "metadata": {},
   "source": [
    "<h2> Now this is a very easy test - its only numeric data. If we add strings and categorical data things can slow down a lot!\n",
    "Let's try this on mixed Astronaut data from Kaggle:</h2>\n",
    "  <a><h3 > https://www.kaggle.com/nasa/astronaut-yearbook</h3></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c729bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Astronauts file\n",
    "\n",
    "fileName ='astronauts.csv'\n",
    "\n",
    "df = pd.read_csv(fileName)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dba890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file\n",
    "df.to_csv('Save.csv' , index = False, float_format= '%.4f')\n",
    "pd.read_csv('Save.csv');\n",
    "\n",
    "# pickle file\n",
    "df.to_pickle('Save.pkl')\n",
    "pd.read_pickle('Save.pkl')\n",
    "\n",
    "\n",
    "# hdf file\n",
    "df.to_hdf('fileName.dhf' , key = 'data', format = 'table')\n",
    "pd.read_hdf('fileName.dhf')\n",
    "\n",
    "\n",
    "# feather file\n",
    "df.to_fth('Save.fth')\n",
    "pd.read_fth('Save.fth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bab46",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db121c",
   "metadata": {},
   "source": [
    "# Recap\n",
    "<section>\n",
    "    \n",
    "   <h3>In terms of file size, HDF5 is the largest for this example. Everything else is approximately equal. For small data sizes,\n",
    "often cv is the easiest as its human readable. HDF5 is great for loading in huge amounts of data quickly. Pickle is\n",
    "faster than CV, but not human readable.\n",
    "Lots of options, don't get hung up on any of them. csv and pickle are easy and for most cases work fine.</h3>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c30dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04324dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
